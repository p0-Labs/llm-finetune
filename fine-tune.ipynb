{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90a7e79e-611c-43a0-8336-99b9ab4a986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch transformers protobuf litgpt\n",
    "import torch\n",
    "from litgpt.model import GPT\n",
    "from litgpt.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84a73093-a9fd-4141-befa-759ede400e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "d_opts = [('cuda', torch.cuda.is_available()), ('mps', torch.backends.mps.is_available()), ('cpu', True)]\n",
    "device = next(device for device, available in d_opts if available)\n",
    "print(f'using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c53be48-dda1-441e-8465-9579f945945f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vllm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/ln/.llama/checkpoints/Llama3.2-1B-Instruct\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[1;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ln/.llama/checkpoints/Llama3.2-1B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vllm'"
     ]
    }
   ],
   "source": [
    "model_path = '/Users/ln/.llama/checkpoints/Llama3.2-1B-Instruct'\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_path = \"/Users/ln/.llama/checkpoints/Llama3.2-1B-Instruct\"\n",
    "\n",
    "# Load model\n",
    "llm = LLM(model=model_path)\n",
    "\n",
    "# Define prompt and sampling params\n",
    "prompt = \"What is the capital of France?\"\n",
    "sampling_params = SamplingParams(max_tokens=50)\n",
    "\n",
    "# Generate output\n",
    "outputs = llm.generate(prompt, sampling_params)\n",
    "\n",
    "# Print response\n",
    "print(outputs[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca66c44e-a223-48c3-af17-9dd57659dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# Specify the path to your model files\n",
    "model_path = \"path/to/your/model/directory\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load the model\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16  # Adjust based on your hardware\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d703ee8-9de0-4ec1-a18e-d4a47e58637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig\n",
    "\n",
    "# Create a config object manually\n",
    "config = LlamaConfig(\n",
    "    vocab_size=32000,  # Adjust based on your tokenizer\n",
    "    hidden_size=2048,  # Adjust based on the 1B model architecture\n",
    "    num_attention_heads=16,\n",
    "    num_hidden_layers=24,\n",
    "    intermediate_size=5504\n",
    ")\n",
    "\n",
    "# Load the model with the manual config\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook_venv",
   "language": "python",
   "name": "notebook_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
